{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] sframe.cython.cy_server: SFrame v2.1 started. Logging /tmp/sframe_server_1519465980.log\n"
     ]
    }
   ],
   "source": [
    "import sframe\n",
    "loans = sframe.SFrame('lending-club-data.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring some features\n",
    "2. Let's quickly explore what the dataset looks like. First, print out the column names to see what features we have in this dataset. On SFrame, you can run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x: +1 if x==0 else -1)\n",
    "loans = loans.remove_column('bad_loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Selecting features\n",
    "In this assignment, we will be using a subset of features (categorical and numeric). The features we will be using are described in the code comments below. If you are a finance geek, the LendingClub website has a lot more details about these features.\n",
    "\n",
    "4. The features we will be using are described in the code comments below. Extract these feature columns and target column from the dataset. We will only use these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'safe_loans'\n",
    "features = ['grade',                     # grade of the loan (categorical)\n",
    "            'sub_grade_num',             # sub-grade of the loan as a number from 0 to 1\n",
    "            'short_emp',                 # one year or less of employment\n",
    "            'emp_length_num',            # number of years of employment\n",
    "            'home_ownership',            # home_ownership status: own, mortgage or rent\n",
    "            'dti',                       # debt to income ratio\n",
    "            'purpose',                   # the purpose of the loan\n",
    "            'payment_inc_ratio',         # ratio of the monthly payment to income\n",
    "            'delinq_2yrs',               # number of delinquincies\n",
    "             'delinq_2yrs_zero',          # no delinquincies in last 2 years\n",
    "            'inq_last_6mths',            # number of creditor inquiries in last 6 months\n",
    "            'last_delinq_none',          # has borrower had a delinquincy\n",
    "            'last_major_derog_none',     # has borrower had 90 day or worse rating\n",
    "            'open_acc',                  # number of open credit accounts\n",
    "            'pub_rec',                   # number of derogatory public records\n",
    "            'pub_rec_zero',              # no derogatory public records\n",
    "            'revol_util',                # percent of available credit being used\n",
    "            'total_rec_late_fee',        # total late fees received to day\n",
    "            'int_rate',                  # interest rate of the loan\n",
    "            'total_rec_int',             # interest received to date\n",
    "            'annual_inc',                # annual income of borrower\n",
    "            'funded_amnt',               # amount committed to the loan\n",
    "            'funded_amnt_inv',           # amount committed by investors for the loan\n",
    "            'installment',               # monthly payment owed by the borrower\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping observations with missing values\n",
    "Recall from the lectures that one common approach to coping with missing values is to skip observations that contain missing values.\n",
    "\n",
    "5. Using SFrame, we run the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans, loans_with_na = loans[[target] + features].dropna_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 29 observations: keeping 122578 \n"
     ]
    }
   ],
   "source": [
    "# Count tne number of rows with missing data\n",
    "num_rows_with_na = loans_with_na.num_rows()\n",
    "num_rows = loans.num_rows()\n",
    "print(\"Dropping %s observations: keeping %s \" % (num_rows_with_na, num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the classes are balanced\n",
    "6. We saw in an earlier assignment that this dataset is also imbalanced. We will undersample the larger class (safe loans) in order to balance out our dataset. We used seed=1 to make sure everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of safe loans         : 0.502247166849\n",
      "Percentage of risky loans         : 0.497752833151\n",
      "Total number of loans in our new dataset : 46503\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == 1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Undersample the safe loans\n",
    "percentage = len(risky_loans_raw)/len(safe_loans_raw)\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed = 1)\n",
    "risky_loans = risky_loans_raw\n",
    "loans_data = risky_loans.append(safe_loans)\n",
    "\n",
    "print(\"Percentage of safe loans         :\", len(safe_loans) / len(loans_data))\n",
    "print(\"Percentage of risky loans         :\", len(risky_loans) / len(loans_data))\n",
    "print(\"Total number of loans in our new dataset :\", len(loans_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "For scikit-learn's decision tree implementation, it numerical values for it's data matrix. This means you will have to turn categorical variables into binary features via one-hot encoding.\n",
    "\n",
    "7. We've seen this same piece of code in earlier assignments. Again, feel free to use this piece of code as is. Refer to the API documentation for a deeper understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_data = risky_loans.append(safe_loans)\n",
    "\n",
    "categorical_variables = []\n",
    "for feat_name, feat_type in zip(loans_data.column_names(), loans_data.column_types()):\n",
    "    if feat_type == str:\n",
    "        categorical_variables.append(feat_name)\n",
    "        \n",
    "for feature in categorical_variables:\n",
    "    loans_data_one_hot_encoded = loans_data[feature].apply(lambda x: {x: 1})\n",
    "    loans_data_unpacked = loans_data_one_hot_encoded.unpack(column_name_prefix=feature)\n",
    "    \n",
    "    # Change Nones to 0s\n",
    "    for column in loans_data_unpacked.column_names():\n",
    "        loans_data_unpacked[column] = loans_data_unpacked[column].fillna(0)\n",
    "    \n",
    "    loans_data.remove_column(feature)\n",
    "    loans_data.add_columns(loans_data_unpacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation\n",
    "8. We split the data into training data and validation data. We used seed=1 to make sure everyone gets the same results. We will use the validation data to help us select model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, validation_data = loans_data.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosted tree classifier\n",
    "Gradient boosted trees are a powerful variant of boosting methods; they have been used to win many Kaggle competitions, and have been widely used in industry. We will explore the predictive power of multiple decision trees as opposed to a single decision tree.\n",
    "\n",
    "We will now train models to predict safe_loans using the features above. In this section, we will experiment with training an ensemble of 5 trees.\n",
    "\n",
    "9.Now, let's use the built-in scikit learn gradient boosting classifier (sklearn.ensemble.GradientBoostingClassifier) to create a gradient boosted classifier on the training data. You will need to import sklearn, sklearn.ensemble, and numpy.\n",
    "\n",
    "You will have to first convert the SFrame into a numpy data matrix. See the API for more information. You will also have to extract the label column. Make sure to set max_depth=6 and n_estimators=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_matrix = train_data.to_numpy()\n",
    "X_train, y_train = train_data_matrix[:, 1:], train_data_matrix[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = GradientBoostingClassifier(n_estimators=5, max_depth=6).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions\n",
    "Just like we did in previous sections, let us consider a few positive and negative examples from the validation set. We will do the following:\n",
    "\n",
    "* Predict whether or not a loan is likely to default.\n",
    "* Predict the probability with which the loan is likely to default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_safe_loans = validation_data[validation_data[target] == 1]\n",
    "validation_risky_loans = validation_data[validation_data[target] == -1]\n",
    "\n",
    "sample_validation_data_risky = validation_risky_loans[0:2]\n",
    "sample_validation_data_safe = validation_safe_loans[0:2]\n",
    "\n",
    "sample_validation_data = sample_validation_data_safe.append(sample_validation_data_risky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_matrix = sample_validation_data.to_numpy()\n",
    "X_sample_validation, y_sample_validation = validation_data_matrix[:, 1:], validation_data_matrix[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1., -1.,  1.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.predict(X_sample_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(y_sample_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15253933,  0.07448605, -0.07747521,  0.18206706])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = model_5.predict_proba(X_sample_validation)\n",
    "proba[:, 1] - proba[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_matrix = validation_data.to_numpy()\n",
    "X_validation, y_validation = validation_matrix[:, 1:], validation_matrix[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = model_5.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66146057733735464"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame()\n",
    "predicted_df['predicted'] = model_5.predict(X_validation)\n",
    "predicted_df['actual'] = y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives = predicted_df[(predicted_df['predicted']==1) & (predicted_df['actual']==-1)]\n",
    "len(false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1491"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives = predicted_df[(predicted_df['predicted']==-1) & (predicted_df['actual']==1)]\n",
    "len(false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
